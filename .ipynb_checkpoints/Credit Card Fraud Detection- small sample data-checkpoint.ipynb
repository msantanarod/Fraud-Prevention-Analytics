{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11abdb4",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ea180",
   "metadata": {},
   "source": [
    "![fraude](fraude.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76fe549",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Important-libraries\" data-toc-modified-id=\"Important-libraries-0\">Important libraries</a></span></li><li><span><a href=\"#Loading-data\" data-toc-modified-id=\"Loading-data-1\">Loading data</a></span></li><li><span><a href=\"#important-functions\" data-toc-modified-id=\"important-functions-2\">important functions</a></span></li><li><span><a href=\"#Understanding-our-data\" data-toc-modified-id=\"Understanding-our-data-3\">Understanding our data</a></span></li><li><span><a href=\"#Correlation-analysis\" data-toc-modified-id=\"Correlation-analysis-4\">Correlation analysis</a></span></li><li><span><a href=\"#Fraud/-non-fraud-ratio\" data-toc-modified-id=\"Fraud/-non-fraud-ratio-5\">Fraud/ non-fraud ratio</a></span></li><li><span><a href=\"#Synthetic-Minority-Oversampling-Technique-(SMOTE)\" data-toc-modified-id=\"Synthetic-Minority-Oversampling-Technique-(SMOTE)-6\">Synthetic Minority Oversampling Technique (SMOTE)</a></span></li><li><span><a href=\"#Traditional-way-of-catching-fraud\" data-toc-modified-id=\"Traditional-way-of-catching-fraud-7\">Traditional way of catching fraud</a></span></li><li><span><a href=\"#XY-Split\" data-toc-modified-id=\"XY-Split-8\">XY Split</a></span></li><li><span><a href=\"#Logistic-Regression-with-imbalance-data\" data-toc-modified-id=\"Logistic-Regression-with-imbalance-data-9\">Logistic Regression with imbalance data</a></span></li><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-10\">Decision Trees</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-11\">Random Forest</a></span></li><li><span><a href=\"#AdaBoostClassifier\" data-toc-modified-id=\"AdaBoostClassifier-12\">AdaBoostClassifier</a></span></li><li><span><a href=\"#Model-Comparison\" data-toc-modified-id=\"Model-Comparison-13\">Model Comparison</a></span></li><li><span><a href=\"#Logistic-Regression-combined-with-SMOTE\" data-toc-modified-id=\"Logistic-Regression-combined-with-SMOTE-14\">Logistic Regression combined with SMOTE</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772480d",
   "metadata": {},
   "source": [
    "## Important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline \n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import dtreeviz.trees\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9cf5c",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072aef5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"Data/creditcard/creditcard_sampledata_3.csv\",index_col=0)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ecc1d",
   "metadata": {},
   "source": [
    "## important functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a8463",
   "metadata": {},
   "source": [
    "def plot_data(X,y):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Data class representation')\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], label='Class #0', alpha=0.5, linewidth=0.15)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], label='Class #1', alpha=0.5, linewidth=0.15, c='r')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df1ab3",
   "metadata": {},
   "source": [
    "## Understanding our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c73f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we have any missing value?\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#According to the dataset description the features from \"V1\" to \"V28\" are the result of PCA(Principal Components Analysis). \n",
    "#We know that features must be scaled before using this technique. \n",
    "#However the features \"Time\" and \"Amount\" are not scaled we should scaled them before continuing with our analysis. \n",
    "\n",
    "df['Amount_scaled'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "#Now we have to drop from our dataset the features \"Time\" and \"Amount\":\n",
    "df=df.drop(columns=['Amount'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838b375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df.columns) \n",
    "df = df[[\n",
    " 'V1',\n",
    " 'V2',\n",
    " 'V3',\n",
    " 'V4',\n",
    " 'V5',\n",
    " 'V6',\n",
    " 'V7',\n",
    " 'V8',\n",
    " 'V9',\n",
    " 'V10',\n",
    " 'V11',\n",
    " 'V12',\n",
    " 'V13',\n",
    " 'V14',\n",
    " 'V15',\n",
    " 'V16',\n",
    " 'V17',\n",
    " 'V18',\n",
    " 'V19',\n",
    " 'V20',\n",
    " 'V21',\n",
    " 'V22',\n",
    " 'V23',\n",
    " 'V24',\n",
    " 'V25',\n",
    " 'V26',\n",
    " 'V27',\n",
    " 'V28',\n",
    " 'Amount_scaled',\n",
    " 'Class']]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab023a0",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[0:]].corr()['Class'][:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(30, 25))\n",
    "    ax = sns.heatmap(corr, mask=mask,cmap='icefire', vmin=-1,vmax=1,annot=True, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0765ecc",
   "metadata": {},
   "source": [
    "## Fraud/ non-fraud ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f135a",
   "metadata": {},
   "source": [
    " the feature \"Class\" is our target variable. This variable has two possible values: \n",
    " 1 for fraudulent transactions and 0 for no fraudulent transactions. \n",
    "A very commun problem in classification datasets is classs imbalance. This means that the dataset contains an imbalance number of fraudulents and no-fraudulents transactions. ML algorithms works better when the different classes are equally represented  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Class'].value_counts()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5474c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio of fraudulent transactions\n",
    "y/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eff27d",
   "metadata": {},
   "source": [
    "We can see that fraudulent transactions represent only 0.9901% of our datasets meanwhile non-fraudulent transactions respresent 99.0099%.\n",
    "it is confirmed we have class imbalance in our dataset. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f768a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualizations can be very usefull to detect the class imbalance:\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "sns.countplot('Class', data=df, palette=\"Set2\")\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e64fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bcb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use a scatter plot to see our class imbalance. \n",
    "#First, we need to convert our dataframe in 2 variables:\n",
    "\n",
    "X=df.iloc[:,0:29].values\n",
    "y=df.Class.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c144b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X: np.ndarray, y: np.ndarray):\n",
    "    sns.set_palette(\"Set2\")\n",
    "    sns.scatterplot(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "    sns.scatterplot(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15)\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea35db0",
   "metadata": {},
   "source": [
    "The plot helps us to see the data imbalance problem very clear. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23067a",
   "metadata": {},
   "source": [
    "##  Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37c3f7",
   "metadata": {},
   "source": [
    "In order to treat the data imbanlance we can use oversampling and undersampling techniques. SMOTE is an oversampling technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X shape: {X.shape}\\ny shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7784b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the resampling method\n",
    "method = SMOTE(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resampled feature set\n",
    "X_sm, y_sm = method.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebccb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(pd.Series(y_sm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resampled data\n",
    "plot_data(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03339d76",
   "metadata": {},
   "source": [
    "## Traditional way of catching fraud\n",
    "First you'll define threshold values using common statistics, to split fraud and non-fraud. Then, use those thresholds on your features to detect fraud. This is common practice within fraud analytics teams.\n",
    "\n",
    "Statistical thresholds are often determined by looking at the mean values of observations. Let's start this exercise by checking whether feature means differ between fraud and non-fraud cases. Then, you'll use that information to create common sense thresholds. Finally, you'll check how well this performs in fraud detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6494d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ec3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a rule for stating which cases are flagged as fraud\n",
    "df['flag_as_fraud'] = np.where(np.logical_and(df['V1'] < -3, df['V7'] < -6), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of flagged fraud cases versus the actual fraud cases\n",
    "print(pd.crosstab(df.Class, df.flag_as_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf14e2",
   "metadata": {},
   "source": [
    "With this first approach we have detected 170 of 492 fraudulent cases, but we got 1226 false positives. Now we will see how we can improve these numbers with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do not need the feature \"flag as fraud\" for this analysis, so I will delete it\n",
    "df=df.drop(columns=['flag_as_fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125d67b",
   "metadata": {},
   "source": [
    "## XY Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Class', axis=1)\n",
    "y=df.Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c46b8",
   "metadata": {},
   "source": [
    "## Logistic Regression with imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and test set (XY split)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.30, random_state=1000,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a69766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use GridSearchCV in order to find the best parameters for our logistic Regression model\n",
    "hyperparameters = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\"solver\" : ['liblinear', 'saga']}\n",
    "\n",
    "randomizedsearch = RandomizedSearchCV(LogisticRegression(), hyperparameters)\n",
    "best_model_random = randomizedsearch.fit(X_train, y_train)\n",
    "print(best_model_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26857488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define our model\n",
    "logreg=LogisticRegression(C=100,solver='liblinear')\n",
    "\n",
    "#fit our the model with our training set\n",
    "logreg.fit(X_train,y_train)\n",
    "# Get predicting values\n",
    "\n",
    "y_predicted_logreg=logreg.predict(X_test)\n",
    "#predicted_y_train=model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print classification report for the test set\n",
    "print(\"Classification report for the test set\")\n",
    "print(classification_report(y_test,y_predicted_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Confusion matrix for the test set\")\n",
    "print(confusion_matrix(y_test, y_predicted_logreg))\n",
    "conf_mat = confusion_matrix(y_test,y_predicted_logreg)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='icefire', fmt='d', cbar=False)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_probs_logreg = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "#Calculate roc_auc_score\n",
    "print(\"roc_auc_score of logistic regression classifier: \",roc_auc_score(y_test, y_pred_probs_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52794e22",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Plot tpr against fpr\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for fraudulent transactions')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f79c4",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41336d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use GridSearchCV in order to find the best parameters for our decision tree\n",
    "hyperparameters = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "              \"min_samples_leaf\": list(range(5,7,1))}\n",
    "\n",
    "randomizedsearch = RandomizedSearchCV(DecisionTreeClassifier(), hyperparameters)\n",
    "best_model_random = randomizedsearch.fit(X_train, y_train)\n",
    "\n",
    "print(best_model_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e164f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define the model with our best parameters and the resampling:\n",
    "dtc= DecisionTreeClassifier(criterion='entropy',min_samples_leaf=5,max_depth=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069230ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##fit our pipeline with our training set\n",
    "dtc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66958709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicting values\n",
    "predicted=dtc.predict(X_test)\n",
    "\n",
    "#predicted_y_train=model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36daf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification report for the test set\n",
    "print(\"Classification report for the test set\")\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba64ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix for the test set\")\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_test,predicted)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='icefire', fmt='d', cbar=False)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93756d2a",
   "metadata": {},
   "source": [
    "performance_model = pd.DataFrame({'Error_metric': ['Accuracy','Precision','Recall'],\n",
    "                               'Train': [accuracy_score(y_train, predicted_y_train),\n",
    "                                         precision_score(y_train, predicted_y_train),\n",
    "                                         recall_score(y_train, predicted_y_train)],\n",
    "                               'Test': [accuracy_score(y_test, predicted_y_test),\n",
    "                                        precision_score(y_test, predicted_y_test),\n",
    "                                        recall_score(y_test, predicted_y_test)]})\n",
    "display(performance_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_probs = dtc.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "#Calculate roc_auc_score\n",
    "print(\"roc_auc_score of decision tree classifier: \",roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59647d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Representation :\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (20,20))\n",
    "\n",
    "plot_tree(dtc,filled = True, rounded=True,feature_names=X.columns, class_names=['No Fraud', \"Fraud\"])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311ac31",
   "metadata": {},
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Plot tpr against fpr\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for fraudulent transactions')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Calculate roc_auc_score\n",
    "print(\"roc_auc_score of logistic regression classifier: \",roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23afa7",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'n_estimators': [1, 30],\n",
    "              'max_features': ['auto', 'log2'], \n",
    "              'max_depth': [4, 8, 10, 12],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "randomizedsearch = RandomizedSearchCV(RandomForestClassifier(), hyperparameters)\n",
    "best_model_random = randomizedsearch.fit(X_train, y_train)\n",
    "\n",
    "print(best_model_random.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model with our best parameters\n",
    "rfc= RandomForestClassifier(criterion='gini',max_depth=8,max_features='log2',n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our pipeline with our training set\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbf54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicting values\n",
    "predictedt=rfc.predict(X_test)\n",
    "\n",
    "#predicted_y_train=model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a015a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification report for the test set\n",
    "print(\"Classification report for the test set\")\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0fa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix for the test set\")\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_test,predicted)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='icefire', fmt='d', cbar=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e433047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_probs = rfc.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "#Calculate roc_auc_score\n",
    "print(\"roc_auc_score of random forest classifier: \",roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rfc.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(7,7))\n",
    "\n",
    "importances_sorted.plot(kind='barh', color='#BCD8C1')\n",
    "plt.title('Features Importances')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871eb88",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdc91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6fc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4544b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00d8127e",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab25f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Logistic Regression\": logreg, \"Decision Trees\": dtc, \"Random Forest\": rfc}\n",
    "results = []\n",
    "\n",
    "# Loop through the models' values\n",
    "for model in models.values():\n",
    "    kf = KFold(n_splits=6, random_state=42, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kf)\n",
    "    results.append(cv_results)\n",
    "plt.boxplot(results, labels=models.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = [ logreg,dtc,rfc]\n",
    "model_names = ['Logistic Regresion','Decision Tree','Random Forest']\n",
    "scores = {}\n",
    "i=0\n",
    "for model in model_pipeline:\n",
    "    mean_score = np.mean(cross_val_score(model, X_train, y_train, cv=5))\n",
    "    scores[model_names[i]] = mean_score\n",
    "    i = i+1\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05e504",
   "metadata": {},
   "source": [
    "## Logistic Regression combined with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define the resampling method and the model the model:\n",
    "resampling = SMOTE()\n",
    "model = LogisticRegression(C=0.1)\n",
    "\n",
    "# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\n",
    "pipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model)])\n",
    "\n",
    "# XY split:\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.30, random_state=1000,stratify=y)\n",
    "\n",
    "\n",
    "#fit the pipeline into the training set:\n",
    "pipeline.fit(X_train, y_train) \n",
    "\n",
    "#Get predictions:\n",
    "predicted_sm = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification report for the test set\n",
    "print(\"Classification report for the test set\")\n",
    "print(classification_report(y_test,predicted_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix for the test set\")\n",
    "print(confusion_matrix(y_test, predicted_sm))\n",
    "conf_mat = confusion_matrix(y_test,predicted_sm)\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cmap='icefire', fmt='d', cbar=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b158f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_probs = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba85188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Plot tpr against fpr\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for fraudulent transactions')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Calculate roc_auc_score\n",
    "print(\"roc_auc_score of logistic regression classifier: \",roc_auc_score(y_test, y_pred_probs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "757px",
    "left": "1501px",
    "top": "123px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
